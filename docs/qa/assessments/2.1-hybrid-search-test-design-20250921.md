# Test Design: Story 2.1

Date: 2025-09-21
Designer: Quinn (Test Architect)

## Test Strategy Overview

- Total test scenarios: 28
- Unit tests: 12 (43%)
- Integration tests: 10 (36%)
- E2E tests: 6 (21%)
- Priority distribution: P0: 8, P1: 12, P2: 8

## Test Scenarios by Acceptance Criteria

### AC1: Implement semantic vector search using Qdrant with cosine similarity ranking

#### Scenarios

| ID           | Level       | Priority | Test                                | Justification                |
| ------------ | ----------- | -------- | ----------------------------------- | ---------------------------- |
| 2.1-UNIT-001 | Unit        | P0       | Query embedding generation accuracy | Pure algorithm logic         |
| 2.1-UNIT-002 | Unit        | P0       | Cosine similarity calculation       | Mathematical computation     |
| 2.1-INT-001  | Integration | P0       | Qdrant vector search with real data | External service integration |
| 2.1-INT-002  | Integration | P1       | Qdrant connection error handling    | Error boundary testing       |
| 2.1-E2E-001  | E2E         | P1       | End-to-end semantic search flow     | Critical user journey        |

### AC2: Implement lexical keyword search using SQLite FTS5 with full-text search indexing

#### Scenarios

| ID           | Level       | Priority | Test                              | Justification             |
| ------------ | ----------- | -------- | --------------------------------- | ------------------------- |
| 2.1-UNIT-003 | Unit        | P0       | FTS5 query construction logic     | Pure query building logic |
| 2.1-UNIT-004 | Unit        | P1       | BM25-like ranking calculation     | Algorithm implementation  |
| 2.1-INT-003  | Integration | P0       | SQLite FTS5 search with real data | Database integration      |
| 2.1-INT-004  | Integration | P1       | FTS5 index performance validation | Performance testing       |
| 2.1-E2E-002  | E2E         | P1       | End-to-end lexical search flow    | Critical user journey     |

### AC3: Combine results using configurable fusion weights (0.6 semantic, 0.4 lexical)

#### Scenarios

| ID           | Level       | Priority | Test                                      | Justification                |
| ------------ | ----------- | -------- | ----------------------------------------- | ---------------------------- |
| 2.1-UNIT-005 | Unit        | P0       | Fusion weight calculation                 | Pure mathematical logic      |
| 2.1-UNIT-006 | Unit        | P0       | Result deduplication by chunk_id          | Algorithm implementation     |
| 2.1-UNIT-007 | Unit        | P1       | Edge cases (empty results, single source) | Boundary condition testing   |
| 2.1-INT-005  | Integration | P0       | Hybrid search with both sources           | Multi-component integration  |
| 2.1-INT-006  | Integration | P1       | Fusion with various result combinations   | Complex integration scenario |

### AC4: Return top 10 results with source metadata and snippets

#### Scenarios

| ID           | Level       | Priority | Test                                 | Justification              |
| ------------ | ----------- | -------- | ------------------------------------ | -------------------------- |
| 2.1-UNIT-008 | Unit        | P1       | Result ranking and sorting           | Pure sorting algorithm     |
| 2.1-UNIT-009 | Unit        | P1       | Snippet generation with highlighting | Text processing logic      |
| 2.1-INT-007  | Integration | P1       | Metadata extraction and formatting   | Data transformation        |
| 2.1-E2E-003  | E2E         | P1       | Complete search result presentation  | User experience validation |

### AC5: Achieve search latency targets (p95 ≤ 1.5s, p99 ≤ 3.0s)

#### Scenarios

| ID          | Level       | Priority | Test                                     | Justification                    |
| ----------- | ----------- | -------- | ---------------------------------------- | -------------------------------- |
| 2.1-INT-008 | Integration | P0       | Performance load testing                 | Critical performance requirement |
| 2.1-INT-009 | Integration | P0       | Latency benchmarking with realistic data | Performance validation           |
| 2.1-E2E-004 | E2E         | P0       | End-to-end performance under load        | Real-world performance testing   |

### AC6: Support query embedding generation for semantic search

#### Scenarios

| ID           | Level       | Priority | Test                                      | Justification              |
| ------------ | ----------- | -------- | ----------------------------------------- | -------------------------- |
| 2.1-UNIT-010 | Unit        | P1       | Sentence Transformers model integration   | External model integration |
| 2.1-INT-010  | Integration | P1       | Embedding generation with various queries | Model performance testing  |

### AC7: Include chunk method and page references in results

#### Scenarios

| ID           | Level       | Priority | Test                            | Justification          |
| ------------ | ----------- | -------- | ------------------------------- | ---------------------- |
| 2.1-UNIT-011 | Unit        | P2       | Metadata formatting logic       | Data formatting logic  |
| 2.1-INT-011  | Integration | P2       | Metadata consistency validation | Data integrity testing |

### AC8: Provide search API endpoint with query parameter

#### Scenarios

| ID           | Level       | Priority | Test                             | Justification           |
| ------------ | ----------- | -------- | -------------------------------- | ----------------------- |
| 2.1-UNIT-012 | Unit        | P0       | Request validation logic         | Input validation logic  |
| 2.1-INT-012  | Integration | P0       | API endpoint integration testing | API contract validation |
| 2.1-E2E-005  | E2E         | P1       | Complete API search workflow     | End-to-end API testing  |

### AC9: Log search queries and performance metrics

#### Scenarios

| ID           | Level       | Priority | Test                           | Justification                  |
| ------------ | ----------- | -------- | ------------------------------ | ------------------------------ |
| 2.1-UNIT-013 | Unit        | P2       | Logging format validation      | Logging logic                  |
| 2.1-INT-013  | Integration | P2       | Metrics collection and storage | Monitoring integration         |
| 2.1-E2E-006  | E2E         | P2       | End-to-end logging and metrics | Complete monitoring validation |

## Risk Coverage

### Critical Risk Mitigation (TECH-001: Vector Search Performance)

- **2.1-INT-008**: Performance load testing
- **2.1-INT-009**: Latency benchmarking
- **2.1-E2E-004**: End-to-end performance under load

### High Risk Mitigation

**PERF-001: Database Query Bottlenecks**

- **2.1-INT-003**: SQLite FTS5 search with real data
- **2.1-INT-004**: FTS5 index performance validation

**TECH-002: Hybrid Fusion Algorithm Complexity**

- **2.1-UNIT-005**: Fusion weight calculation
- **2.1-UNIT-006**: Result deduplication by chunk_id
- **2.1-INT-005**: Hybrid search with both sources
- **2.1-INT-006**: Fusion with various result combinations

**SEC-001: Query Injection Vulnerabilities**

- **2.1-UNIT-012**: Request validation logic
- **2.1-INT-012**: API endpoint integration testing

## Recommended Execution Order

1. **P0 Unit tests** (fail fast)

   - 2.1-UNIT-001: Query embedding generation accuracy
   - 2.1-UNIT-002: Cosine similarity calculation
   - 2.1-UNIT-003: FTS5 query construction logic
   - 2.1-UNIT-005: Fusion weight calculation
   - 2.1-UNIT-006: Result deduplication by chunk_id
   - 2.1-UNIT-012: Request validation logic

2. **P0 Integration tests**

   - 2.1-INT-001: Qdrant vector search with real data
   - 2.1-INT-003: SQLite FTS5 search with real data
   - 2.1-INT-005: Hybrid search with both sources
   - 2.1-INT-008: Performance load testing
   - 2.1-INT-009: Latency benchmarking
   - 2.1-INT-012: API endpoint integration testing

3. **P0 E2E tests**

   - 2.1-E2E-004: End-to-end performance under load

4. **P1 tests** (core functionality)

   - All remaining P1 scenarios in order

5. **P2 tests** (as time permits)
   - All P2 scenarios for complete coverage

## Test Data Requirements

### Vector Search Test Data

- Sample documents with known semantic relationships
- Various query types (simple, complex, ambiguous)
- Edge cases (empty queries, very long queries)

### Lexical Search Test Data

- Documents with specific keyword patterns
- Various text formats and languages
- Special characters and symbols

### Performance Test Data

- Large document collections (1000+ documents)
- Realistic query volumes (100+ concurrent queries)
- Various query complexities

## Test Environment Requirements

### Unit Tests

- Mock external dependencies (Qdrant, SQLite)
- Isolated test environment
- Fast execution (< 1 second per test)

### Integration Tests

- Real Qdrant instance (test database)
- Real SQLite database with test data
- Network connectivity for external services

### E2E Tests

- Complete system environment
- Realistic data volumes
- Performance monitoring tools

## Success Criteria

### Unit Tests

- 100% code coverage for core algorithms
- All edge cases covered
- Fast execution (< 1 second per test)

### Integration Tests

- All external service integrations working
- Error handling validated
- Performance baselines established

### E2E Tests

- Critical user journeys validated
- Performance targets met
- Complete system functionality verified

## Test Maintenance Considerations

- Tests should be independent and isolated
- Use test data factories for consistent test data
- Implement proper cleanup after each test
- Monitor test execution time and optimize as needed
- Regular review of test coverage and effectiveness
